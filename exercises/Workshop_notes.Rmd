---
title: "Workshop notes"
output:
  html_document:
    df_print: paged
    theme: flatly
    toc: yes
    toc_depth: 3
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# EEG analysis in R

R is among the fastest growing, most popular statistics programming languages. It is open source, free, and enormously extensible through the vast number of freely available packages. To date, it is not very frequently used for EEG analysis, and the small number of dedicated EEG packages that already exist are not well developed. Nevertheless, there are many positive aspects to R that make it a desirable tool to use for EEG analysis.

##  Basic R concepts
In this workshop, I assume a basic level of programming knowledge but no extensive knowledge of R specifically. Those of you familiar with Matlab or Python will not find anything particularly difficult to grasp. Nevertheless, to begin with, here are a few important basics.

As in most languages, simple mathematical operations work just fine.
`r 2 + 2`

The output of an operation can be assigned to a variable/object using the <- operator. Note that the equals sign (=) also works, but is not recommended.

```{r assignment_op}
a <- 2 + 2
a
```
A vector is a variable with multiple values all of the same type (e.g. all integers).
```{r vector}
a <- 1:5
a
```

The beating heart of R is the *data frame* - a collection of multiple related vectors. Here are the first five rows of a built in example dataset, **iris**

```{r df_example}
head(iris)
```
The data frame can have any number of columns and rows. Each column can be data of any type, so a data frame can have a mixture of different data types - characters, integers, doubles. A special data type in R is the *factor*. A factor is exactly what you'd expect - a variable indicating to which categorical level of a factor the observations in a given row belongs.

Most statistical commands in R operate on data frames or individual vectors.

A simple way to access a single column from a data frame is using the $ operator.

```{r dollar_op}
iris$Sepal.Length
```

Data frames (and matrices) can also be accessed using subsetting with square brackets ([row(s) , column]).

## RStudio

[RStudio]("https://www.rstudio.com") is an Integrated Development Environment for R, and is a vast improvement over the one included with R itself. Its features include:
* Project management
* Package manager
* Github integration
* Autocomplete

## Installing and loading packages

Packages are the way you add features to R. As of today, there are 10754 packages available on the CRAN central repository that cover a huge range of capabilites. Some packages are already installed; we'll download others as we go on. Packages are loaded using *library()*. First let's load *tidyverse*.

```{r load_tidyverse}
library(tidyverse)
```

The tidyverse is actually a collection of several packages that include functions for plotting and data manipulation. Many of the things these packages implement are also possible in base R, but are often much more intuitive and simpler through the tidyverse.

* ggplot2
  + Advanced plotting functions
* dplyr
  + Data manipulation (in the good sense!)
* purrr
  + Functional programming and iteration through loops
  
We'll be using these packages extensively.

Packages can be installed using the RStudio GUI or from the console or within a script (note: don't do this). For example, to install the *brms* package from CRAN:

```
install.packages("brms")
```

Some packages are hosted on Github, not CRAN. For example, my EEG package (which is very much in early development and nowhere near ready for prime-time) is hosted on Github. These can be installed using install_github() from the *devtools* package.

```
devtools::install_github("craddm/eegUtils")
```

## Getting EEG data into R {.tabset .tabset-fade}

.mat files from recent versions of Matlab can be saved in HDF5 format (file format -v7.3 in Matlab terms). Fieldtrip outputs .mat files by default, and EEGLAB .set files are simply .mat files with a different extension. Several packages exist for reading HDF5 files directly in R (h5, rhdf5). In most cases, it is simpler, however, to export and import data as text, as importing from .mat files can have some irritating quirks. We'll cover reading from hdf5 and earlier .mat files under the Fieldtrip tab below.

### From EEGLAB

EEGLAB offers export to text through its GUI and the Matlab command line. Some data is already supplied today, but when producing this yourself, ensure that the data is output transposed such that each column is an electrode, and each row a timepoint. We'll use read_delim from *readr* to load an example file. Note that you can use the "Import Dataset" icon in your RStudio environment to do this. Note that you may get a bunch of warning messages when doing this - they can be safely ignored.

```{r load_first, warning = FALSE}
ObjLock <- read_delim("../data/ObjLockS1two.csv", "\t",
                           escape_double = FALSE, trim_ws = TRUE)
head(ObjLock)
```

Column X1 is the timepoint for each sample. Each column contains the amplitude for each of 64 electrodes. In total, there are 375 trials. There is an extra, blank column, that is something to do with how readr interprets the CSV output by EEGLAB. Note that several things are missing that we would like. First, the first column does not have a name. Second, there is no epoch indicator. Third, there is nothing to tell us which trial belongs to which condition. 

The first two issues can be dealt with easily - I'll offer some dplyr solutions. Base R solutions are also possible, but I won't go into them here. Columns can be renamed using rename() (e.g. rename(new_name = old_name, new_name2 = old_name2,...)). Columns we don't want can be dropped using select(). We do not necessarily know exactly how many trials we have, but that can easily be figured out. We know that each trial contains only one of each timepoint, and each row in the dataframe is one timepoint from one trial. If we group the dataframe by time, we essentially split it into separate dataframes for each timepoint, with each row then coming from consecutive trials. We can then simply label each row appropriately. We use group_by() to group by time, and then mutate to add a new column for trial number. Within mutate, we use n(), a command which counts the number of rows in each group.

```{r fix_up_data}
ObjLock <- ObjLock %>%
  rename(times = X1) %>%
  select(-X66) %>%
  group_by(times) %>%
  mutate(trial_no = 1:n())

head(ObjLock)
```

Finally, for many purposes we'll want the data to be in long, tidy format rather than wide format. We'll use tidyr::gather(). 

```{r gather_obj}
ObjLock <- ObjLock %>%
  gather(electrode, amplitude, -times, -trial_no)

head(ObjLock)
```

Note that you could easily combine all of these steps into a single chain of commands using the pipe operator (*%>%*).

```{r combi, warning= FALSE}
ObjLock <- read_delim("../data/ObjLockS1two.csv", "\t",
                           escape_double = FALSE, trim_ws = TRUE) %>%
  rename(times = X1) %>%
  select(-X66) %>%
  group_by(times) %>%
  mutate(trial_no = 1:n()) %>%
  gather(electrode, amplitude, -times, -trial_no)

head(ObjLock)
```

### From Fieldtrip/.mat

Fieldtrip files are simply .mat files. Recently exported .mat files in v7.3/HDF5 format need one of the HDF5 R packages - h5 or rhdf5. While h5 is available through CRAN, rhdf5 is available from Bioconductr, another source of R packages. Unfortunately, however, Fieldtrip uses cell arrays to store data in its data structure, and existing HDF5 tools in R cannot read them correctly. It is simpler to save .mat files in older Matlab formats (e.g. v6), and load them using the *R.matlab* package.

```{r older_matlab, message = FALSE}
library(R.matlab)
ft_data <- readMat("../data/Subj12ObjPresFu.mat")
ft_data
```

Getting the data into a sensible format is then a matter of examining the structure of the file and extracting the right elements. The imported file is a list of lists, with the information we want all within the **data** list. In this file, Fieldtrip's data was stored in a cell array called trial, with each cell containing one trial of data. Trial is the 4th item in the data list, and is thus accessed using [[4]]. We unlist this so that we are left with a single vector. "times" is  also a list with the timepoints for each trial, accessed with [[6]]. Electrode labels are in the "labels" list, accessed with [[1]]. Unfortunately for us, the data dimensions are not directly given in the file. But we know there are 70 channels (the length of labels), 69 trials (the length of the trial list), and if we were to check the number of unique entries in the time vector, it would be 1024. The simplest thing is to organize the data as a 2x2 matrix, with each electrode as a columns and each rows a single timepoint from a single trial.

```{r extract_from_mat}
elecs <- unlist(ft_data$data[[1]])
raw_ft <- unlist(ft_data$data[[4]])
times <- unlist(ft_data$data[[6]])
dim(raw_ft) <- c(70, 70656)
final_ft <- data.frame(cbind(t(raw_ft), times))
names(final_ft) <- c(elecs, "times")
head(final_ft)
```

Finally, we'll convert this to long format and use the same trick as in the EEGLAB section to add trial numbers.

```{r ft_to_long}
final_ft <- final_ft %>%
  gather(electrode, amplitude, -times) %>%
  group_by(times) %>%
  mutate(trial_no = 1:n())

head(final_ft)
```

Just to prove that this is converting data to the right format, here is a very simple plot showing the ERP at one channel. I'll show you how to make these plots elsewhere.

```{r simp_ERP, echo = FALSE, fig.height = 3}
final_ft %>%
  filter(times >= -.1 & times <= .4 & electrode == "Oz") %>%
  group_by(times) %>%
  summarise(amplitude = mean(amplitude)) %>%
  ggplot(aes(x = times, y = amplitude)) + geom_path()
```


### From MNE-python

MNE-python is the simplest of all. MNE-Python data can easily be turned into a Pandas data frame and saved as either a .csv or *feather* file. A Pandas data frame is already very close to the ideal format for use in R, and should already contain most of the information we need.

.csv is the most platform agnostic. *feather* is a lightweight binary format that enables easy transfer between Python and R, and potentially other languages. Note that it's not recommended for long-term storage - it is still in active development and the file format may change. Essentially, once the data is in it should be no different whether it was from a CSV or a feather file.

```{r from_mne, message = FALSE}
mne_data <- read_csv('../data/mne_data.csv', progress = FALSE) %>%
  gather(electrode,amplitude, -condition, -epoch, -time) 

head(mne_data)
```

The data is already in long format, and has separate columns for epoch, time, electrode, amplitude, and experimental condition. Note that the design here was 2 X 2 - trials could have a light, a touch, both light and touch, or no stimulus at all. MNE nests conditions such that each factor is separated by a forward slash. Also coded here is behavioural response (yes or no). We need condition to be split into multiple columns, one for each factor. Again, tidyr has the answer. Using separate() we can split a column into multiple columns.

```{r split_cols}
mne_data <- separate(mne_data,condition, c("light","touch","report"), sep = "/")
head(mne_data)
```

## Re-organizing data

First up let's load some data. These are some simple ERPs from one electrode for a group of participants, for two conditions.

```{r load_ERPs}
levCatGA <- read.csv("https://raw.githubusercontent.com/craddm/ExploringERPs/master/levCatObjNon.csv",
                     header = FALSE)
names(levCatGA) <- c("Object", "Non-Object", "Time", "Subject")
head(levCatGA)
```

Let's re-arrange this data so that amplitude is in one column and Category (Object and Non-Object) is in another. We want to have one row be one observation and each column be a variable.

We'll use filter() to select rows within a defined time range. We'll use mutate to change Subject to a factor. Note this could also be done separately in base R, but I like to keep things in the pipe. Finally we'll use gather() to bring the data together.


```{r reorganize}
levCatGA <- levCatGA %>%
  filter(Time >= -100 & Time <= 400) %>%
  mutate(Subject = factor(.$Subject)) %>%
  gather(Category, amplitude, -Time, -Subject) 

head(levCatGA)
```

## Basic timecourse plotting

```{r basic_ERP_plot}
levCat.plot <- ggplot(levCatGA,aes(Time,amplitude)) + #set up basic ggplot object
  scale_color_brewer(palette = "Set1") + #define a simple colour scale
  theme_classic() #define "theme"

levCat.plot+
  stat_summary(fun.y = mean, geom = "line", size = 1, aes(colour = Category))+
  labs(x = "Time (ms)", y = expression(paste("Amplitude (", mu, "V)")), colour = "")+
  geom_vline(xintercept = 0, linetype = "dashed" )+
  geom_hline(yintercept = 0, linetype = "dashed")
```

Note that the plot is built up in *layers*. Each element is plotted in the order it appears in the code, so if you need to ensure something goes in front of something else, it needs to go after it in the code.

Anyway, we can do better than that. How about some 95% confidence intervals?

```{r ci_erps, fig.height = 4}
levCat.plot+
  stat_summary(fun.data = mean_cl_normal,geom = "ribbon",alpha = 0.2, aes(fill = Category)) +
  stat_summary(fun.y = mean, geom = "line", size = 1, aes(colour = Category)) +
  labs(x = "Time (ms)", y = expression(paste("Amplitude (", mu, "V)")), colour = "") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed")
```
We can even plot individual lines for each subject.

```{r indiv_lines, fig.height = 4}
levCat.plot+
  geom_line(aes(group = interaction(Subject,Category)), alpha = 0.2)+
  stat_summary(fun.data = mean_cl_normal,geom = "ribbon",alpha = 0.2, aes(fill = Category)) +
  stat_summary(fun.y = mean, geom = "line", size = 1.5, aes(colour = Category)) +
  labs(x = "Time (ms)", y = expression(paste("Amplitude (", mu, "V)")), colour = "") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed")
```

## How to run group statistics

### Pre-specified windows

In many circumstances, you will have a priori electrodes and time-windows of interest. Suppose that we want to study mean amplitude in the P1 time window (90-110 ms). We can use dplyr::filter() to choose only those rows that meet these critera. We then use group_by() to group the data by subject and category. Finally, we use summarise() to calculate the mean amplitude. This leaves us with a data frame containing mean amplitudes for each subject for each condition. Finally, we'll run a paired t-test on the data, storing the result.

```{r single_test}
stat_test <- levCatGA %>%
  filter(Time >= 90 & Time <= 110 & Category != "difference") %>%
  group_by(Category, Subject) %>%
  summarise(amplitude = mean(amplitude))

stat_test
test_result <- t.test(amplitude~Category, data = stat_test, paired = TRUE)
test_result
```
Note that test_result is a list object with individual elements accessed using the $ operator or [] subsetting. 

```{r query_object}
str(test_result)
test_result$p.value
test_result$statistic
test_result[4]
```

Helpfully, the output of many stats objects can be tidied up and turned into a dataframe using *tidy* from the *broom* package, which makes them easier to use for subsequent plotting (see next section).

```{r}
broom::tidy(test_result)
```

### Mass univariate tests

Often we want to run a whole series of tests across multiple timepoints, electrodes, frequency bands etc.

Each command in R operates on a single data frame. If we need to run a test on each timepoint, we need to separate dataframes for each timepoint.

The nest() function from tidyr rearranges the data in this way.

```{r}
time_nest <- nest(levCatGA, -Time)
head(time_nest)
```

Why is this useful? This makes it simple iterate through each timepoint using **map** commands from *purrr*. Map command (map, map_dbl, map_df) take a list as input and perform a function on each element of that list. We'll add an extra column to our nested data frame containing the output of the t.test() function applied to each element of the *data* list within the time_nest frame.

```{r run_t_tests}
time_nest <- mutate(time_nest,
                    stats = map(data, ~t.test(amplitude ~ Category,
                                              paired = TRUE, data = .x)))
head(time_nest)
```

We now have a data frame with the results of the t.test function for each timepoint. Suppose you now want to retrieve the t.test results for a specific timepoint. You can access list elements using double brackets [[]]; you would need to find the right row number for the timepoint you want, and enclose that in the brackets.

```{r query_nest}
time_nest$stats[[10]]
```

Alternatively, we can use map to iterate through the list and extract, for example, each p-value.

```{r get_pvals}
time_nest %>% 
  mutate(pvals = map_dbl(stats, "p.value")) 
```
Alternatively, we can use broom::tidy() to convert our statistical test results into a data frame. Then we remove the original data using select() and unnest() to get back a data frame with a column for each element of the statistical test.

```{r levStats}
stat_out <- levCatGA %>%
  nest(-Time) %>%
  mutate(stats = (map(data, ~broom::tidy(t.test(amplitude~Category, paired = TRUE, data = .x))))) %>%
  select(-data) %>%
  unnest()
stat_out
```

Plot the difference and confidence intervals as calculated by the t-test command.

```{r stats_plot, warning = FALSE}
levCat.plot+
  geom_line(data = stat_out, aes(x = Time, y = estimate)) +
  geom_ribbon(data = stat_out, aes(ymax = conf.high, ymin = conf.low, x = Time, y = estimate), alpha = 0.3) +
  #stat_summary(data = stat_out, fun.data = mean_cl_normal,geom = "line",alpha = 0.2, aes(x= Time, y = estimate)) +
  stat_summary(fun.y = mean, geom = "line", size = 1, aes(colour = Category)) +
  labs(x = "Time (ms)", y = expression(paste("Amplitude (", mu, "V)")), colour = "") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed")
```

Then add a line indicating which time points are significantly different.

```{r add_sig, warning = FALSE, message = FALSE}
stat_out$corr.p <- p.adjust(stat_out$p.value, method = "holm")
stat_out$p.sig <- 0 + (stat_out$p.value < .05)
stat_out$p.sig[stat_out$p.sig == 0] <- NA
stat_out

levCat.plot+
  geom_line(data = stat_out, aes(x = Time, y = estimate)) +
  geom_ribbon(data = stat_out, aes(ymax = conf.high, ymin = conf.low, x = Time, y = estimate), alpha = 0.3) +
  geom_line(data = stat_out, aes(x = Time, y = p.sig-3), size = 2)+
  #stat_summary(data = stat_out, fun.data = mean_cl_normal,geom = "line",alpha = 0.2, aes(x= Time, y = estimate)) +
  stat_summary(fun.y = mean, geom = "line", size = 1, aes(colour = Category)) +
  labs(x = "Time (ms)", y = expression(paste("Amplitude (", mu, "V)")), colour = "") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_hline(yintercept = 0, linetype = "dashed")

```

## Topography plotting

Topographies are an essential tool for visualizing EEG data. In order to plot them effectively in R, we need to be able to map from electrode labels to electrode positions in Cartesian space. First, I'll load in a basic set of electrode locations generated from EEGLAB. .ced files generated by EEGLAB have X, Y, and Z co-ordinates that locate the electrodes in 3D space, but need to be projected into cartesian space for 2d plotting. It's simplest to do that from the theta and radius values which are also provided.

```{r leipzig_elecs, warning = FALSE, message = FALSE, fig.height = 3}
chan_locs_biosemiLE <- read.delim("../data/chan_locs_biosemiLE.ced")
ggplot(chan_locs_biosemiLE,
                    aes(X, Y, label = labels))+
  geom_text()+
  theme_bw()+
  coord_equal()


chan_locs_biosemiLE$radianTheta <- pi/180*chan_locs_biosemiLE$theta

chan_locs_biosemiLE <- chan_locs_biosemiLE %>%
  mutate(x = .$radius*sin(.$radianTheta),
         y = .$radius*cos(.$radianTheta))

ggplot(chan_locs_biosemiLE,
                    aes(x, y, label = labels))+
  geom_text()+
  theme_bw()+
  coord_equal()
```

Here are some other electrode co-ordinates from a standard Biosemi 64 layout.

```{r orig_elecs, message = FALSE, fig.height=3}
electrodeLocs <- read_delim("https://raw.githubusercontent.com/craddm/ExploringERPs/master/biosemi70elecs.loc", 
                            "\t",
                            escape_double = FALSE,
                            col_names = c("chanNo", "theta", "radius", "electrode"),
                            trim_ws = TRUE)

head(electrodeLocs)

electrodeLocs$radianTheta <- pi/180*electrodeLocs$theta

electrodeLocs <- electrodeLocs %>%
  mutate(x = .$radius * sin(.$radianTheta),
         y = .$radius * cos(.$radianTheta))

ggplot(electrodeLocs, aes(x, y, label = electrode))+
  geom_text()+
  theme_bw()+
  coord_equal()
```

Let's add a simple headshape to our plot.

```{r head_shape}
theme_topo <- function(base_size = 12)
  {
  theme_bw(base_size = base_size) %+replace%
      theme(
            rect             = element_blank(),
            line             = element_blank(),
            axis.text = element_blank(),
            axis.title = element_blank()
           )
}

circleFun <- function(center = c(0,0),diameter = 1, npoints = 100) {
  r = diameter / 2
  tt <- seq(0,2*pi,length.out = npoints)
  xx <- center[1] + r * cos(tt)
  yy <- center[2] + r * sin(tt)
  return(data.frame(x = xx, y = yy))
}

headShape <- circleFun(c(0, 0), round(max(electrodeLocs$x)), npoints = 100) # 0
nose <- data.frame(x = c(-0.075, 0, .075),y=c(.495, .575, .495))

ggplot(headShape, aes(x, y))+
  geom_path()+
  geom_text(data = electrodeLocs,
            aes(x, y, label = electrode))+
  geom_line(data = nose,
            aes(x, y, z = NULL))+
  theme_topo()+
  coord_equal()
```

Now we have our template head, and we want to start plotting our data.

The simplest thing to do is to join our location and data file together so that each data point has an associated location. We'll use *left_join()* from dplyr.

```{r join_dat_locs, message = FALSE}
topotest <- read_csv("https://raw.githubusercontent.com/craddm/ExploringERPs/master/topographyTest.csv") %>%
  gather(electrode, amplitude, -Times)
topotest$amplitude <- as.double(topotest$amplitude)
allData <- topotest %>% left_join(electrodeLocs, by = "electrode")
head(allData)
```

Now let's plot the amplitude at each electrode.

```{r first_plot, warning = FALSE, fig.height = 3}
library(scales)

#Define Matlab-style Jet colourmap
jet.colors <- colorRampPalette(c("#00007F", "blue", "#007FFF", "cyan", "#7FFF7F", "yellow", "#FF7F00", "red", "#7F0000"))

#select a Timepoint
singleTimepoint <- filter(allData,Times == 170.90)

#Draw our map!
ggplot(headShape, aes(x, y))+
  geom_path(size = 1.5)+
  geom_point(data = singleTimepoint, aes(x, y, colour = amplitude), size = 3)+
  scale_colour_gradientn(colours = jet.colors(10), guide = "colourbar", oob = squish)+ #note: oob = squish forces everything outside the colour limits to equal nearest colour boundary (i.e. below min colours = min colour)
  geom_line(data = nose,aes(x, y, z = NULL), size = 1.5)+
  theme_topo()+
  coord_equal()
```

To get our normal topographical plots, we'll need to interpolate between the electrodes to create a surface.

To make this a little simpler, I have created an package that includes a function which will create topographical plots. Please note that this has not been widely tested and is very much under development! Code can be accessed from my [Github account]("https://github.com/craddm"), and an explainer can be found on [my blog]("https://craddm.github.io"). The function, topoplot(), takes as input any data frame with the columns x, y, and amplitude. It also requires a column "time" in order to choose specific timepoints for plotting; otherwise, it averages over all supplied timepoints. Note that amplitude need not actually be amplitude, and can be any arbitrary measure. 

```{r interp_plot, warning = FALSE, message = FALSE, fig.height= 3}
library(eegUtils)

allData %>%
  rename(time = Times) %>%
  topoplot(allData, timepoint = 250)
```

Other colour scales can easily be applied; for example, we can use the viridis colour scale by adding + scale_fill_viridis() after the initial call.

```{r viri_cols,fig.height = 3, message = FALSE}
library(viridis)
allData %>%
  rename(time = Times) %>%
  topoplot(allData, timepoint = 250, clim = c(-2,2)) + scale_fill_viridis()

```

By default, the function uses the same interpolation method as EEGLAB. An alternative is to fit a non-linear smooth to the surface using a Generalized Additive Model. In principle this approach could be extended to model differences between conditions.

```{r gam_topo,fig.height = 3, message = FALSE}
allData %>%
  rename(time = Times) %>%
  topoplot(allData, timepoint = 250, method = "gam")

```

## Frequency analysis

R has several possible commands to perform FFTs. Here we'll use the base R method *spectrum()*. In combination wit 

```{r stats_spectrum}
spec_psd <- mne_data %>%
  nest(time, amplitude) %>%
  mutate(psd = map(data, ~spectrum(.x$amplitude, plot = FALSE)))

spec_psd_two <- spec_psd %>%
  mutate(freqs = map(.$psd,"freq"), spec = map(.$psd, "spec")) %>%
  select(-data, -psd) %>%
  unnest()

spec_psd_two %>%
  mutate(freqs = freqs*128) %>%
  filter(freqs <= 40 & freqs >= 4) %>%
  group_by(freqs, electrode, light) %>%
  summarise(spec = mean(spec)) %>%
  ggplot(aes(x = freqs, y= log10(spec), colour = electrode)) + 
  geom_line() + 
  theme_classic() +
  facet_wrap(~light)

spec_psd_two %>%
  mutate(freqs = freqs*128) %>%
  filter(freqs <= 40 & freqs >= 4) %>%
  group_by(freqs, electrode, light) %>%
  summarise(spec = mean(spec)) %>%
  ggplot(aes(x = log(freqs), y= log10(spec), colour = electrode)) + 
  geom_line() + 
  theme_classic() +
  facet_wrap(~light)

```

## Multivariate pattern analysis/machine learning

For machine learning, the most critical package in R is *caret*. It provides a simple interface to over 200 different modelling functions. Here I'll be using it as an interface to *glmnet*, a package that runs lasso and elastic-net regularized generalized linear models. 

```{r load_caret, message = FALSE}
library(caret)
```

We'll use it with the MNE data from above. That data is from an extperiment with four trial types - trials with a light, a touch, both, or neither. We'll use a classifier at every time point to see when exactly the method can accurately classify trials as light or no light trials.

First let's check what proportion of trials are in each category. We use table() to calculate how many rows (note that in this case, this is the totals at every timepoint, electrode, and trial, so is much higher than the number of trials alone). We convert that to proportions using prop.table()

```{r proportions}
table(mne_data$light)
prop.table(table(mne_data$light))
```

Note that the full design was balanced, but this is after artefact rejection. ~47% of trials have a light, ~53% have no light. A classifier that classifies trials as having a light more than 47% of the time is doing better than chance.

The main to use caret is through the train() command. We tell it here we want to use glmnet, it to use binomial classification, it pre-process data by dividing it by its standard devation. Many other options are available. The first argument to train() is the regression formula, specified in standard R format: dv ~ iv. Note that here I use "." as a shortcut that passes all dataframe columns as predictors (other than the dv), after first dropping irrelevant columns. *glmnet* does not respect factors as expected, so we first have to convert our long format data to wide format using spread(). Other methods operate fine with factors, so this is not always necessary.

spread() is the opposite of gather() - you specify which factor column you want to split across multiple columns, and which column contains the data that should fill each cell.

```{r spread_out}
mne_data <- mne_data %>% 
  spread(electrode,amplitude)
head(mne_data)
```

trainControl() is used to pass various parameters that influence how the classifier is constructed and evaluated. These can include telling it to use methods such as K-fold cross-validation, bootstrapping, out-of-bag estimation. Here we will tell to use K-fold cross-validation with 5 folds. Essentially, it randomly partitions the data into a training and test (by default, using an 80% of the data to train and 20% as test) set, trains the model and uses it to classify the test set. It repeats this five times. By default it uses the accuracy of the model's predictions to choose the right tuning parameters for the model. Other options are available (for example, Receiver Operating Characteristic/Area under the Curve).

We'll take advantage of nest() to arrange the data into a collection of separate dataframes for each timepoint and then map() to create a classifier for each timepoint.

```{r run_ML, cache = TRUE, message = FALSE, warning = FALSE}
fitControl <- trainControl(method = "cv",
                           number = 5)

mne_classifier <- mne_data %>%
  select(-report,-touch,-epoch) %>%
  nest(-time) %>%
  mutate(fit = map(data, ~train(light ~ ., data = .x,
                                preProcess = c("center","scale"),
                                method = "glmnet", family = "binomial",
                                trControl = fitControl)))
```

Our output dataframe now has a column containing the classifier output at each timepoint, which can be accessed using [[]].

```{r class_output}
head(mne_classifier)
mne_classifier$fit[[1]]
```

To extact performance at each timepoint, we'll use a bunch of map() commands from purrr to iterate through the "fit" list. The successive map commands: 
1. Extract performance measures of the final model for each fold.
2. Extract Accuracy for the final model of each fold.
3. Average across those accuracy measures to produce a single value per timepoint.

We then plot the accuracy at each timepoint.

```{r plot_ml}
data.frame(Accuracy = map(mne_classifier$fit,"resample") %>%
                   map("Accuracy") %>%
                   map_dbl(~mean(.x)),
           time = mne_classifier$time) %>%
  ggplot(aes(time, Accuracy))+ geom_line() + theme_classic()
```

## Github repository

All code from this workshop can be found at https://github.com/craddm/EEG_Workshop

For further details on many aspects of plotting and analysis, check my blog at https://craddm.github.io